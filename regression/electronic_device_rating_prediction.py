# -*- coding: utf-8 -*-
"""Electronic Device Rating Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1k393fkYN0r6ddXHKr3w1uqp_BBL3E-8j

# **Step 1: Reading & Exploring Data**
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import missingno as msno
from sklearn.svm import SVC
from sklearn import metrics
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import OrdinalEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold,cross_val_score
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.metrics import accuracy_score, classification_report

# %matplotlib inline

df = pd.read_excel('/content/ElecDeviceRatingPrediction.xlsx')

df.head()

df.shape

df.info()

df.describe()

df.groupby('brand')['Price'].mean().plot(kind='bar')
plt.xlabel('Brand')
plt.ylabel('Average Price')
_ = plt.title('Price by Brand')

# @title Distinct Values of Categorical Features
object_columns = df.select_dtypes(include='object').columns
for column in object_columns:
    print(f"Column: {column}")
    print(df[column].unique(),'\n')
distinct_values_dict = {}
for column in object_columns:
    distinct_values_dict[column] = df[column].unique()

categorical_columns = df.select_dtypes(include=['object', 'category']).columns

# Plot unique values in each categorical column
for column in categorical_columns:
    plt.figure(figsize=(8, 6))
    df[column].value_counts().plot(kind='bar')
    plt.title(f'Unique Values in {column}')
    plt.xlabel(column)
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)
    plt.show()

"""# **Step 2: Preprocessing**

## **Duplicates Handling**

Check if there are duplicate records
"""

print(df.duplicated().sum())

df = df.drop_duplicates()

msno.matrix(df)

"""The previous matrix shows that there are no null values in the dataset"""

# Check if there're null values in the data
df.isna().sum()

df[['os-bits', 'os']] = df['os'].str.split('-bit', n=1, expand=True)


df['os'] = df['os'].str.strip()

df['os-bits'] = pd.to_numeric(df['os-bits'])

#converting categorical values into numerical values.
#There are some columns that have categorical values ,but actually a numerical value

df['ram_gb'] = df['ram_gb'].astype(str).str.replace('GB','')
df['ram_gb'] = pd.to_numeric(df['ram_gb'])

df['ssd'] = df['ssd'].astype(str).str.replace('GB','')
df['ssd'] = pd.to_numeric(df['ssd'])

df['hdd'] = df['hdd'].astype(str).str.replace('GB','')
df['hdd'] = pd.to_numeric(df['hdd'])


df['graphic_card_gb'] = df['graphic_card_gb'].astype(str).str.replace('GB','')
df['graphic_card_gb'] = pd.to_numeric(df['graphic_card_gb'])

df.head()

categorical_df = df.select_dtypes(include=['object', 'category'])
categorical_df.head()

numerical_df = df.select_dtypes(include=['float64', 'int64'])
numerical_df.head()

#numerical = df[['ram_gb',	'ssd',	'hdd',	'graphic_card_gb',	'Price',	'Number of Ratings',	'Number of Reviews',	'os-bits']]
numeric_mean = {}
numeric_mean = {c: df[c].mean() for c in numerical_df}

numeric_mean

#categorical = df[['brand', 'processor_brand', 'processor_name', 'processor_gnrtn', 'ram_type', 'os', 'weight', 'warranty',	'Touchscreen',	'msoffice']]
categorical_mode  = {}
categorical_mode  = {c: df[c].mode()[0] for c in categorical_df}
categorical_mode.pop('rating', None)

categorical_mode

"""> Even though there're no null values in our dataset, there's still missing data in the column 'processor_gnrtn'

## **Feature Encoding**

> **1. Ordinal encoder will be applied to the following ordinal features:**

> *   rating
>*   processor_gnrtn
>*   warranty
>*   processor_name
"""

ordinal_encoder_rating = OrdinalEncoder(categories=[['1 star', '2 stars', '3 stars', '4 stars', '5 stars']])
df['rating'] = ordinal_encoder_rating.fit_transform(df[['rating']])


ordinal_encoder_processor_gnrtn = OrdinalEncoder(categories=[['Not Available','4th', '7th','8th','9th', '10th','11th','12th']])
df['processor_gnrtn']=ordinal_encoder_processor_gnrtn.fit_transform(df[['processor_gnrtn']])

ordinal_encoder_warranty = OrdinalEncoder(categories=[['No warranty', '1 year', '3 years', '2 years'] ])
df['warranty']=ordinal_encoder_warranty.fit_transform(df[['warranty']])

ordinal_encoder_processor_name = OrdinalEncoder(categories=[['Celeron Dual', 'Pentium Quad', 'Ryzen 3', 'Core i3', 'Ryzen 5','Ryzen 7','Core i5','Core i7','Ryzen 9','Core i9','M1']])
df['processor_name']=ordinal_encoder_processor_name.fit_transform(df[['processor_name']])

"""

> **2. Label Encoding will be applied to the following nominal features:**


> *   brand
*   processor_brand
*   ram_type
*   os
*   weight







"""

label_encoder_brand = LabelEncoder()
df['brand'] = label_encoder_brand.fit_transform(df['brand'])

label_encoder_processor_brand = LabelEncoder()
df['processor_brand'] = label_encoder_processor_brand.fit_transform(df['processor_brand'])

label_encoder_ram_type = LabelEncoder()
df['ram_type'] = label_encoder_ram_type.fit_transform(df['ram_type'])

label_encoder_os = LabelEncoder()
df['os'] = label_encoder_os.fit_transform(df['os'])

label_encoder_weight = LabelEncoder()
df['weight'] = label_encoder_weight.fit_transform(df['weight'])

"""

> **3. One-Hot Encoding will be applied to the following binary features:**

> *   msoffice
*   Touchscreen







"""

# Identify categorical columns
categorical_cols = ['msoffice']

# # Apply one-hot encoding
# df = pd.get_dummies(df, columns=categorical_cols)

# Apply one-hot encoding
one_hot_encoder_msoffice = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
one_hot_encoded_array = one_hot_encoder_msoffice.fit_transform(df[categorical_cols])

# Get the feature names for the new columns
one_hot_encoded_columns = one_hot_encoder_msoffice.get_feature_names_out(input_features=categorical_cols)

# Create a DataFrame from the one-hot encoded array with the new column names
one_hot_encoded_df = pd.DataFrame(one_hot_encoded_array, columns=one_hot_encoded_columns, index=df.index)

# Concatenate the one-hot encoded DataFrame with the original DataFrame
df = pd.concat([df, one_hot_encoded_df], axis=1)

# Drop the original categorical columns
df.drop(categorical_cols, axis=1, inplace=True)

# Identify categorical columns
categorical_cols = ['Touchscreen']

# # Apply one-hot encoding
# df = pd.get_dummies(df, columns=categorical_cols)


# Apply one-hot encoding
one_hot_encoder_touchscreen = OneHotEncoder(handle_unknown='ignore', sparse_output=False)
one_hot_encoded_array = one_hot_encoder_touchscreen.fit_transform(df[categorical_cols])

# Get the feature names for the new columns
one_hot_encoded_columns = one_hot_encoder_touchscreen.get_feature_names_out(input_features=categorical_cols)

# Create a DataFrame from the one-hot encoded array with the new column names
one_hot_encoded_df = pd.DataFrame(one_hot_encoded_array, columns=one_hot_encoded_columns, index=df.index)

# Concatenate the one-hot encoded DataFrame with the original DataFrame
df = pd.concat([df, one_hot_encoded_df], axis=1)

# Drop the original categorical columns
df.drop(categorical_cols, axis=1, inplace=True)

df['msoffice_Yes'] = df['msoffice_Yes'].astype(int)
df['Touchscreen_Yes'] = df['Touchscreen_Yes'].astype(int)
df['msoffice_No'] = df['msoffice_No'].astype(int)
df['Touchscreen_No'] = df['Touchscreen_No'].astype(int)

df.head()

# Count the repetitions of a certain value in the column
value_counts = df['processor_gnrtn'].value_counts().get(0,0)

print(value_counts)
print("The rows with missing data represents: ",(value_counts/float(df.shape[0]) * 100).round(3) ,"% of the data.")

"""## **Outliers Handling**"""

numerical_data = df[['Number of Ratings', 'Number of Reviews', 'Price']]
plt.figure(figsize=(12, 8))
sns.boxplot(data=numerical_data, orient="h")
plt.show()

Q1 = df.quantile(0.25)
Q3 = df.quantile(0.75)
IQR = Q3 - Q1

iqr_threshold = 3

iqr_ranges = {'Q1': Q1, 'Q3': Q3, 'IQR': IQR, 'Thresh' : iqr_threshold}

outlier_rows = df[((df < (Q1 - iqr_threshold * IQR)) | (df > (Q3 + iqr_threshold * IQR))).any(axis=1)]


for column in numerical_data:
    median_value = df[column].median()
    df.loc[df[column] < (Q1[column] - iqr_threshold * IQR[column]), column] = median_value
    df.loc[df[column] > (Q3[column] + iqr_threshold * IQR[column]), column] = median_value

df.info()

df.describe()

"""## **Feature Scaling**"""

#Normalization
features_to_normalize=['Price']
price_scaler = MinMaxScaler(feature_range=(0, 1))

df[features_to_normalize] = price_scaler.fit_transform(df[features_to_normalize])

df.head()

features_to_normalize=['Number of Ratings']
num_rating_scaler = MinMaxScaler(feature_range=(0, 1))

df[features_to_normalize] = num_rating_scaler.fit_transform(df[features_to_normalize])

df.head()

features_to_normalize=['Number of Reviews']
num_reviews_scaler = MinMaxScaler(feature_range=(0, 1))

df[features_to_normalize] = num_reviews_scaler.fit_transform(df[features_to_normalize])

df.head()

"""# **Step 3: Feature Engineering**"""

df['Popularity'] = df['Number of Ratings'] + df['Number of Reviews']
df.head()

"""# **Step 4: Feature Selection**"""

numerical_df.head()

df.info()

"""## **ANOVA + Chi-Squared**"""

numerical_df['msoffice_No']=df['msoffice_No']
numerical_df['msoffice_Yes']=df['msoffice_Yes']
numerical_df['Touchscreen_No']=df['Touchscreen_No']
numerical_df['Touchscreen_Yes']=df['Touchscreen_Yes']

numerical_df.head()

categorical_df.head()
encoded_categorical_df=df[['brand'	,'processor_brand',	'processor_name'	,'processor_gnrtn'	,'ram_type',	'os'	,'weight'	,'warranty','rating']]

from scipy.stats import f_oneway

target_column = 'rating'


feature_columns = [col for col in numerical_df.columns if col != target_column]

alpha = 0.05
significant_features = []

for feature_column in feature_columns:
    groups = [df[df[feature_column] == value][target_column] for value in df[feature_column].unique()]
    statistic, p_value = f_oneway(*groups)

    print(f"Feature: {feature_column}")
    print("ANOVA statistic:", statistic)
    print("p-value:", p_value)

    if p_value < alpha:
        print("Reject null hypothesis: There are significant differences between at least two groups.")
        significant_features.append(feature_column)
    else:
        print("Fail to reject null hypothesis: There is no significant difference between the groups.")

    print()

print("Significant features:", significant_features)

encoded_categorical_df.info()

nan_count = sum(encoded_categorical_df.isna().any())
print("Number of columns with NaN values:", nan_count)

nan_values = df.isna().any()

# Print columns with NaN values, if any
columns_with_nan = nan_values[nan_values].index.tolist()
if len(columns_with_nan) > 0:
    print("Columns with NaN values:", columns_with_nan)
else:
    print("No NaN values found in the dataset.")

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

X = encoded_categorical_df.drop(columns=['rating'])
y = encoded_categorical_df[target_column]

# Perform chi-squared test to select k best features
k = 2
chi2_selector = SelectKBest(chi2, k=k)
X_kbest = chi2_selector.fit_transform(X, y)


selected_indices = chi2_selector.get_support(indices=True)


selected_features = X.columns[selected_indices]

print("Selected features:", selected_features)

"""## **Mutual Information**"""

from sklearn.feature_selection import mutual_info_classif


X = encoded_categorical_df.drop(columns='rating')
y = encoded_categorical_df[target_column]


mi = mutual_info_classif(X, y)


mi_df = pd.DataFrame({'Feature': X.columns, 'Mutual Information': mi})

# Sort the DataFrame by mutual information values in descending order
mi_df = mi_df.sort_values(by='Mutual Information', ascending=False)

print(mi_df)

"""## **Correlation**"""

corr_matrix = df.corr()

plt.figure(figsize=(20, 10))
sns.heatmap(corr_matrix, cmap="YlGnBu",annot=True,fmt='.2f')
plt.title('Correlation Heatmap')
plt.show()

corr_matrix['rating'].sort_values(ascending=False)

corr_matrix = df.corr()
corr_matrix['rating'].sort_values(ascending=False)

df.head()

df.nunique()

df.describe()

"""**Insights**

There are some outliers in all the 3 numerical columns

No missing values in the dataset
"""

df.head()

df.info()

corr_matrix['rating'].sort_values(ascending=False)

"""# **Step 5: Modelling**

## **Dataset Splitting**

Trial 3
"""

X = df[['Number of Ratings','Number of Reviews','msoffice_Yes','msoffice_No','warranty','os-bits','ssd','processor_name',"hdd"]]
Y = df['rating']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

"""## **Models Without Validation**

### **1. Linear Regression**
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error

model = LinearRegression()


model.fit(X_train, y_train)

train_predictions = model.predict(X_train)

# Round predictions to the nearest integer (if rating is treated as classification)
train_predictions_rounded = [round(pred) for pred in train_predictions]


train_accuracy = accuracy_score(y_train, train_predictions_rounded)
print("Training Accuracy:", train_accuracy)

test_predictions = model.predict(X_test)

test_predictions_rounded = [round(pred) for pred in test_predictions]


test_accuracy = accuracy_score(y_test, test_predictions_rounded)
print("Test Accuracy:", test_accuracy)

mse = mean_squared_error(y_test, test_predictions_rounded)
print("Mean squared error:", mse)

"""### **2. Random Forest Regressor**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score


max_r2_score = -1

for x in range(200):
    rf = RandomForestRegressor(random_state=x)
    rf.fit(X_train, y_train)
    Y_pred_rf = rf.predict(X_test)
    current_r2_score = r2_score(y_test, Y_pred_rf)
    if current_r2_score > max_r2_score:
        max_r2_score = current_r2_score
        best_x = x

rf = RandomForestRegressor(random_state=best_x)
rf.fit(X_train, y_train)
Y_pred_rf = rf.predict(X_test)
r2_rf = r2_score(y_test, Y_pred_rf)
print("The R^2 score achieved using RandomForestRegressor is: ", r2_rf)

mse_rf = mean_squared_error(y_test, Y_pred_rf)
print("The Mean Squared Error achieved using RandomForestRegressor is: ", mse_rf)

"""## **Models With Validation**"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.30, shuffle=True, random_state=10)

"""### **1. Linear Regression**"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_squared_error, r2_score



linear_model = LinearRegression()
linear_model.fit(X_train, y_train)


y_test_pred = linear_model.predict(X_test)


mse_test = mean_squared_error(y_test, y_test_pred)

r_squared_test = r2_score(y_test, y_test_pred)

print("Mean Squared Error (MSE) on test set:", mse_test)
print("R-squared (R²) on test set:", r_squared_test)

# Scatter plot for test set
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred, color='green', label='Test Data')
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', label='Line of Best Fit')
plt.title('Test Set: Actual vs. Predicted')
plt.xlabel('Actual Values')
plt.ylabel('Predicted Values')
plt.legend()
plt.grid(True)
plt.show()

"""### **2. Random Forest Regressor**"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt
from sklearn.metrics import mean_squared_error, r2_score

regressor = RandomForestRegressor(n_estimators=200, max_depth=3, random_state=42)

regressor.fit(X_train, y_train)

y_pred = regressor.predict(X_test)


mse = mean_squared_error(y_test, y_pred)
print("Mean squared error:", mse)

r2 = r2_score(y_test, y_pred)
print("R2" , r2)

test_predictions_rounded = [round(pred) for pred in y_pred]

"""### **3. Ridge**"""

from sklearn.linear_model import Ridge

ridge_reg = Ridge(alpha=0.0001)

ridge_reg.fit(X_train, y_train)

y_pred_ridge = ridge_reg.predict(X_test)


mse_ridge = mean_squared_error(y_test, y_pred_ridge)
r2_ridge = r2_score(y_test, y_pred_ridge)
print("Ridge Regression MSE:", mse_ridge)
print("Ridge Regression R2:", r2_ridge)

"""### **4. Lasso**"""

from sklearn.linear_model import Lasso
from sklearn.metrics import mean_squared_error, r2_score


lasso_reg = Lasso(alpha=0.0001)

lasso_reg.fit(X_train, y_train)


y_pred_lasso = lasso_reg.predict(X_test)


mse_lasso = mean_squared_error(y_test, y_pred_lasso)


r2_lasso = r2_score(y_test, y_pred_lasso)

print("Lasso Regression MSE:", mse_lasso)
print("Lasso Regression R2:", r2_lasso)

"""# **Step 6: Pickling**"""

import pickle

"""### **1. Pickle Models**"""

pickle.dump(regressor, open('randomForest.pkl', 'wb'))
pickle.dump(linear_model, open('linearRegression.pkl', 'wb'))
pickle.dump(ridge_reg, open('ridgeRegression.pkl', 'wb'))

"""### **2. Pickle Encoding**"""

ordinal_encoders = {
    'OE_processor_gnrtn' : ordinal_encoder_processor_gnrtn,
    'OE_warranty' : ordinal_encoder_warranty,
    'OE_processor_name' : ordinal_encoder_processor_name,
}

label_encoders = {
    'LE_brand' : label_encoder_brand,
    'LE_processor_brand' : label_encoder_processor_brand,
    'LE_ram_type' : label_encoder_ram_type,
    'LE_os' : label_encoder_os,
    'LE_weight' : label_encoder_weight
}

oneHot_encoders = {
    'HE_msoffice' : one_hot_encoder_msoffice,
    'HE_touchscreen' : one_hot_encoder_touchscreen,
}

#pickling ordinal encoding models
pickle.dump(ordinal_encoders, open('ordinalEncoding.pkl', 'wb'))

#pickling label encoding
pickle.dump(label_encoders, open('labelEncoding.pkl', 'wb'))

#pickling onehot encoding
pickle.dump(oneHot_encoders, open('oneHotEncoding.pkl', 'wb'))

"""### **3. Pickle Normalization**"""

normalization_fns = {
    'normalize_price': price_scaler,
    'normalize_num_rating' : num_rating_scaler,
    'normalize_num_reviews' : num_reviews_scaler
}

#pickling normalization
pickle.dump(normalization_fns, open('normalizationFns.pkl', 'wb'))

"""### **4. Pickle Constant Values**"""

preproc_values = {
    'column_means' : numeric_mean,
    'column_modes' : categorical_mode
}

#pickling const values
pickle.dump(preproc_values, open('preprocValues.pkl', 'wb'))

pickle.dump(distinct_values_dict, open('distinctValues.pkl', 'wb'))